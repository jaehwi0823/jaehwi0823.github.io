---
layout: article
title: Spectral Normalization for Generative Adversarial Networks review
tags: [paper, review, ml, SNGAN, GAN]
---

# Spectral Normalization for Generative Adversarial Networks (2018.02)

<br> 2018.09.09
<br> Jaehwi Park

## 1. Introduction

> - GAN은 매우 복잡한 확률분포를 학습할 수 있는 능력때문에 인기가 많았지만,이론적인 탄탄함도 인기에 한 몫을 했음
> - 2016~2017의 많은 논문에 따르면, Discriminator 학습을 위한 좋은 estimator를 찾는 것이 중요함
> - 좋은 estimator: Model Dist.와 Target Dist. 간의 Density ratio를 잘 나타내야 함
> - 이 방법이 분포를 모르더라도 variational optimization 을 수행하는 _implicit models_ 
---
> - 매우 고차원 공간에서는 discriminator의 Density ratio estimation이 보통 부정확하고 불안정함
> - 또한, target dist.의 multimodal structure를 학습하는 것도 어려움
> - Even worse when the support of the model dist. & support of the target dist. are disjoint, Model dist.와 Target dist.를 완벽하게 구분하는 D가 존재한다고 함.
---
> SNGAN은 다음과 같은 장점이 있다고 주장합니다.
> - Lipschitz constant가 유일한 hyper-parameter임
> - 구현이 간단하고 추가적은 연산이 매우 적음

## 2. Method

<br>Consider a simple discriminator<br>
> $D = f(x,\theta)$, <br>
> - where $\theta := {W^{1}, ..., W^{L+1}}$ <br> 
> - $W^{l} \in \mathbb{R}^{d_l \times d_{l-1}}$ <br>
> - $a_l$ is an activation function <br>
>
> $D(x, \theta)=\mathcal{A}(f(x,\theta))$

> Standard formulation 
> - $D^{*}_G(x)=\frac{q_{data}(x)}{q_{data}(x) + p_G(x)}=sigmoid(f^{*}(x))$
> - and, its derivative can be unbounded or even incomputable

> A particularly successful works in 2017 proposed methods to control the Lipschitz
> - proposed methods to control the Lipschitz constant of the discriminator by adding regularization terms __defined on input examples x__
> - $||f||_{Lip}$ = Smallest Value M s.t. $||f(x)-f(x')||/||x-x'|| ≤ M$ for any $x, x'$ with the norm being L2 norm.
> - input based regularization은 samples를 이용해서 수식이 쉽지만,
> - Generator의 supports의 바깥 공간에 대해서는 정규화를 적용할 수 없음... 그래서 Heuristic한 방법이 적용됨

### 2.0 prerequisite

1) NORM (Mathmatics)
> - 수학에서 NORM이란 vector space에서 모든 vector에 대해 무조건 0이상의 양수 길이(or 크기)를 할당하는 함수
> - Norm은 scalability and additivity 등의 요건을 꼭 만족해야 함
> - Simple Example
>   - Absolute-value norm: $||x|| = |x|$ <br>
>   - Euclidean norm: $||x||_2 := \sqrt[2]{x_1^2+\dots+x_n^2}$

2) Matrix Norm
> - norm on the vector space $K^{m \times n}$
> - $|| . ||$ : $K^{m \times n}$ → $\mathbb{R}$ 

### 2.1 Spectral Normalization

SN은 D의 모든 Layers에 Spectral Norm을 적용시켜 Lipschitz constant를 조절합니다.
> - By definition, $||g||_{Lip} = SUP_h \sigma(\bigtriangledown g(h))$
> - where, $\sigma (A)$ is spectral norm of the matrix A (L2 matrix norm of A)
>  - $\sigma (A)$는 singular value의 최대값이라고 함
> - g : $h_{in} → h_{out}$ 에 대해,
>  - $||g||_{Lip} = SUP_h \sigma(\bigtriangledown g(h)) = SUP_h \sigma(W) = \sigma(W)$
> - $||g_1 \circ g_2||_{Lip} ≤ ||g_1||_{Lip} \cdot ||g_2||_{Lip}$ 성질에 의하여, 
>  - activation function $a_l$의 $||a_l||_{Lip}=1$ 이라면, (대표적으로 Relu, LeakyRelu가 해당 조건을 만족)
>  - $||f||_{Lip} ≤ \displaystyle\prod_{l=1}^{L+1}\sigma(W^l)$

그래서 결론적으로는..
> - each weight matrix $W^l$에 대해
> - $\bar{W}_{SN}(W) := W/\sigma(W)$ 연산으로 정규화를 해주면 됩니다 :)

### 2.2 Fast Approximation of the Spectral Norm $\sigma(W)$

> - Naive하게 SVD를 계산해서 SN을 적용시키는 것는 too Heavy!
> - 그래서 Power Iteration Method를 사용

### 2.3 Gradient Analysis of the SN Weights

> - SN을 적용시킨 것과 그렇지 않은 것의 W 미분값을 비교해보면,
> - SN을 적용시켰을때, Gradient의 방향성이 더 널리 퍼지도록 panelty를 주는 역할을 수행한다고 합니다.

## 3. SPECTRAL NORMALIZATION VS OTHER REGULARIZATION TECHNIQUES

### 3.1 Weight Normalization
> 불필요한데 매우 강한 Regularization 요건이 추가돼서 성능 향상에 방해가 됨

### 3.2 Orthonormal Regularization
> 불필요한 차원 (Noise와 같은) 의 정보도 강화시켜서 올바른 정보의 서열화 스펙트럼을 망가뜨린다고 함

### 3.3 WGAN-GP
> Input sample에 국한되어, Heuristic한 방법으로 Lipschitz Constraints를 적용함 → BAD
