---
layout: article
title: Event2Mind review
tags: [paper, review, ml, e2m]
---

# Event2Mind: Commonsense Inference on Events, Intents, and Reactions


## 1. Introduction

> 본 논문은 짧은 자유형식 이벤트 텍스트에서, 그 이벤트 참가자의 의도 및 반응을 유추하는 네트워크에 대한 연구입니다.

![Figure1](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Figure1.png)

> 주요 Contribution은 다음과 같습니다.
> - 인간의 상식추론에 활용가능한 새로운 "Corpus Dataset"
> - 이벤트에 직접 언급되지 않은 주변인에 대한 추론
> - 의도 및 반응에 대한 __"텍스트 설명" 생성__ 모델링

## 2. Dataset

> 데이터셋을 위해 stories, blogs, and Wiktionary idioms 등에서 이벤트 문구를 수집함

> 또한, 본 논문에서는 Variables 개념을 도입
> - Person X, Person Y
> - Person X eats ____ for dinner
> - Frequency가 어느정도 확보되면, "Person X eats pasta for dinner" 처럼 그냥 활용
> - 그러나 only person mentions 만 바꾸고 나머지는 future research에 맡김

![Table1](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Table1.png)

### 2.1 Event Extraction

![Table2](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Table2.png)

> 다음과 같은 corpus를 모두 활용함
> - The ROC Story training set (Mostafazadeh et al., 2016)
> - The Google Syntactic N-grams (Goldberg and Orwant, 2013)
> - The Spinn3r corpus (Gordon and Swanson, 2008)

> - 모형의 균형을 위해 Content words의 숫자를 조절함
> - 확실히 labelled될 수 있는 수준으로는 남김: 적어도 2개 / 많아도 5개
> - "get up"과 같은 Phrasal word도 하나의 단어로 인식
> - 자주 일어나는 Events만 선별함 (아래의 기준)

![Table5](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Table5.png)

### 2.2 Croudsourcing

> - we periodically removed workers with high disagreement rates, at our discretion

![Figure8](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Figure8.png)

> __Coreference among Person variables__
> - Event에 여러 사람이 등장하는 경우 말이 안되는 예들이 생김
> - 예) PersonX punches PersonX’s lights out
> - 이런 케이스들(8,406 events)을 제외함.. (from 17,806 events)

> 모든 Conference의 경우 3명의 annotator가 할당됨
> Overall k=0.4

![Cohen](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/cohen.png)
https://m.blog.naver.com/PostView.nhn?blogId=y4769&logNo=220680837692&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F

## 3. Models

![Figure3](https://raw.githubusercontent.com/jaehwi0823/jaehwi0823.github.io/master/_image/E2M/Figure3.png)<br>

> - Input은 variables를 활용한 자유형식 텍스트: PersonX gives PersonY ____ as a gift
> - 이벤트 E를 워드 임베딩의 시퀀스로 표현: <$e_1$, $e_2$, $...$, $e_n$> $\in$ $\mathbb{R}^{n\times D}$
> - 워드 임베딩 시퀀스는 Vector $h_E$로 임베딩됨: $h_E \in \mathbb{R}^H$

> __Encoding Events__
> - Encoding Function: $f$
> > - MAX and Mean Pooling
> > - ConvNet
> > - bi-directional RNN (GRU)
> - Decoder set-ups
> > - n-gram re-ranking
> > - Sequence Genenration using RNN


```python

```
